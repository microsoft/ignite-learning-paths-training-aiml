{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the ABSA Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Install aditional pip packages on your Compute instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/NervanaSystems/nlp-architect.git@absa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy==2.1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Download Notebooks, Training Data, Training / Inference scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml\n",
    "from azureml.core import Workspace, Datastore, Experiment, Environment, Model\n",
    "import urllib.request\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will open an device login prompt. Login with your credentials that have access to the workspace.\n",
    "\n",
    "# Connect to the workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"Using workspace:\",ws.name,\"in region\", ws.location)\n",
    "\n",
    "# Connect to the default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "print(\"Datastore:\",ds.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "Path(\"dataset\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"notebooks\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"scripts\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"temp\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will take some time to run as it is downloading a large dataset plus code files. Please allow around 10-15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files needed\n",
    "base_link = \"https://raw.githubusercontent.com/microsoft/aiml40/master/aiml40/absa/\"\n",
    "\n",
    "# Download Data \n",
    "if not Path(\"dataset/glove.840B.300d.zip\").is_file():\n",
    "    urllib.request.urlretrieve('http://nlp.stanford.edu/data/glove.840B.300d.zip', 'dataset/glove.840B.300d.zip')\n",
    "\n",
    "urllib.request.urlretrieve(base_link+'dataset/clothing_absa_train.csv', 'dataset/clothing_absa_train.csv')\n",
    "urllib.request.urlretrieve(base_link+'dataset/clothing-absa-validation.json', 'dataset/clothing-absa-validation.json')\n",
    "urllib.request.urlretrieve(base_link+'dataset/clothing_absa_train.csv', 'dataset/clothing_absa_train_small.csv')\n",
    "\n",
    "# Download Notebooks\n",
    "urllib.request.urlretrieve(base_link+'notebooks/absa-hyperdrive.ipynb', 'notebooks/absa-hyperdrive.ipynb')\n",
    "urllib.request.urlretrieve(base_link+'notebooks/absa.ipynb', 'notebooks/absa.ipynb')\n",
    "\n",
    "# Download Scripts \n",
    "urllib.request.urlretrieve(base_link+'scripts/score.py', 'scripts/score.py')\n",
    "urllib.request.urlretrieve(base_link+'scripts/train.py', 'scripts/train.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to the data store\n",
    "ds.upload('dataset', target_path='clothing_data', overwrite=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3 - Setup AMLS\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"absa-cluster\"\n",
    "\n",
    "try:\n",
    "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Using compute cluster:', cluster_name)\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           vm_priority='lowpriority',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=8)\n",
    "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    cluster.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
