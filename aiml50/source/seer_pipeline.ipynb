{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Azure ML SDK Version:  1.0.60\n"]}],"source":"import azureml\nfrom azureml.core import Workspace, Experiment, Datastore, Environment\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.data.datapath import DataPath, DataPathComputeBinding\nfrom azureml.data.data_reference import DataReference\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\nfrom azureml.pipeline.steps import PythonScriptStep, EstimatorStep\nfrom azureml.widgets import RunDetails\nfrom azureml.train.estimator import Estimator\nimport os\n\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Setup Variables"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"os.environ['STORAGE_ACCOUNT_KEY'] = 'YourAccountKeyHere'"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"datastorename='seerdata'\ndatastorepath='hardware'\ncontainername='seer-container'\nstorageaccountname='aiml50setupstorage'\nstorageaccountkey=os.environ.get('STORAGE_ACCOUNT_KEY')\ncomputetarget='twtcluster'"},{"cell_type":"markdown","metadata":{},"source":["# Register/Reference a Datastore"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'workspaceblobstore': <azureml.data.azure_storage_datastore.AzureBlobDatastore object at 0x7f926c220630>, 'workspacefilestore': <azureml.data.azure_storage_datastore.AzureFileDatastore object at 0x7f926c228048>, 'damoseerdata': <azureml.data.azure_storage_datastore.AzureBlobDatastore object at 0x7f926c228550>}\n"]}],"source":"# workspace\nws = Workspace.from_config(\n    path='./azureml-config.json')\nprint(ws.datastores)"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":"# See if that datastore already exists and unregister it if so\ntry:\n    datastore = ws.datastores[datastorename]\n    print ('Unregistering existing datastore')\n    datastore.unregister()\nexcept:\n    print ('Data store doesn\\'t exist, no need to remove')\nfinally:\n    # register the datastore\n    datastore = Datastore.register_azure_blob_container(workspace=ws,\n                                        datastore_name=datastorename,\n                                        container_name=containername,\n                                        account_name=storageaccountname,\n                                        account_key=storageaccountkey,\n                                        create_if_not_exists=True)\n\nprint('Datastore registered: ', datastore)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<azureml.data.azure_storage_datastore.AzureBlobDatastore object at 0x7f926c220278>\n","AmlCompute(workspace=Workspace.create(name='damo-mlworkspace', subscription_id='bc202ec2-54ef-4576-b7fb-a961c983398e', resource_group='damo-aiml'), name=damoseercompute, id=/subscriptions/bc202ec2-54ef-4576-b7fb-a961c983398e/resourceGroups/damo-aiml/providers/Microsoft.MachineLearningServices/workspaces/damo-mlworkspace/computes/damoseercompute, type=AmlCompute, provisioning_state=Succeeded, location=australiaeast, tags=None)\n"]}],"source":"# data\ndatastore = ws.datastores['seerdata']\ndatareference = DataReference(datastore=datastore, \n                    data_reference_name=\"seerdata\", \n                    path_on_datastore=datastorepath)\n"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Create Compute Resources"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"try:\n    cpu_cluster = ComputeTarget(workspace=ws, name=computetarget)\n    print('Found existing cluster, use it.')\nexcept ComputeTargetException:\n    compute_config = AmlCompute.provisioning_configuration(\n        vm_size='STANDARD_NC12', \n        min_nodes=1, \n        max_nodes=4)\n    cpu_cluster = ComputeTarget.create(ws, computetarget, compute_config)\n\ncpu_cluster.wait_for_completion(show_output=True)\ncompute = ws.compute_targets[computetarget]\n\nprint('Compute registered: ', compute)"},{"cell_type":"markdown","metadata":{},"source":"# Define Pipeline!\n\nThe following will be created and then run:\n\n  1. Pipeline Parameters\n  2. Data Process Step\n  3. Training Step\n  4. Model Registration Step\n  5. Pipeline registration\n  6. Submit the pipeline for execution\n"},{"cell_type":"markdown","metadata":{},"source":"## Pipeline Parameters\nWe need to tell the Pipeline what it needs to learn to see!"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(<azureml.pipeline.core.graph.PipelineParameter object at 0x7f926c23e320>, <azureml.data.datapath.DataPathComputeBinding object at 0x7f926c23e358>)\n"]}],"source":"datapath = DataPath(datastore=datastore, path_on_datastore=datastorepath)\ndata_path_pipeline_param = (PipelineParameter(name=\"data\", \n                                             default_value=datapath), \n                                             DataPathComputeBinding(mode='mount'))\nprint(data_path_pipeline_param)\n\n# Configuration for data prep and training steps\ndataprepEnvironment = Environment.from_pip_requirements('dataprepenv', 'requirements-dataprepandtraining.txt')\ndataprepRunConfig = RunConfiguration()\ndataprepRunConfig.environment = dataprepEnvironment"},{"cell_type":"markdown","metadata":{},"source":["## Data Process Step"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<azureml.pipeline.steps.python_script_step.PythonScriptStep object at 0x7f926c2205f8>\n"]}],"source":"seer_tfrecords = PipelineData(\n    \"tfrecords_set\",\n    datastore=datastore,\n    is_directory=True\n)\n\nprepStep = PythonScriptStep(\n    'parse.py',\n    source_directory='.',\n    name='Data Preparation',\n    compute_target=compute,\n    arguments=[\"--source_path\", data_path_pipeline_param, \"--target_path\", seer_tfrecords],\n    runconfig=dataprepRunConfig,\n    inputs=[data_path_pipeline_param],\n    outputs=[seer_tfrecords]\n)\n\nprint(prepStep)"},{"cell_type":"markdown","metadata":{},"source":["## Training Step"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n","WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"]},{"name":"stdout","output_type":"stream","text":["<azureml.pipeline.steps.estimator_step.EstimatorStep object at 0x7f926c228320>\n"]}],"source":"seer_training = PipelineData(\n    \"train\",\n    datastore=datastore,\n    is_directory=True\n)\n\ntrain = Estimator(source_directory='.',\n                    compute_target=compute,\n                    entry_script='train.py',\n                    pip_requirements_file='requirements-dataprepandtraining.txt')\n\ntrainStep = EstimatorStep(\n    name='Model Training',\n    estimator=train,\n    estimator_entry_script_arguments=[\"--source_path\", seer_tfrecords, \n                                    \"--target_path\", seer_training,\n                                    \"--epochs\", 5,\n                                    \"--batch\", 10,\n                                    \"--lr\", 0.001],\n    inputs=[seer_tfrecords],\n    outputs=[seer_training],\n    compute_target=compute\n)\n\nprint(trainStep)"},{"cell_type":"markdown","metadata":{},"source":["# Register Model Step"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<azureml.pipeline.steps.python_script_step.PythonScriptStep object at 0x7f926c23ebe0>\n"]}],"source":"registerEnvironment = Environment.from_pip_requirements('registerenv', 'requirements-registration.txt')\nregisterRunConfig = RunConfiguration()\nregisterRunConfig.environment = registerEnvironment\n\nseer_model = PipelineData(\n    \"model\",\n    datastore=datastore,\n    is_directory=True\n)\n\nregisterStep = PythonScriptStep(\n    'register.py',\n    source_directory='.',\n    name='Model Registration',\n    arguments=[\"--source_path\", seer_training, \n               \"--target_path\", seer_model],\n    inputs=[seer_training],\n    outputs=[seer_model],\n    compute_target=compute,\n    runconfig=registerRunConfig\n)\n\nprint(registerStep)"},{"cell_type":"markdown","metadata":{},"source":["## Create and publish the Pipeline"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n","WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n","WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"]},{"name":"stdout","output_type":"stream","text":["Created step Data Preparation [245a32d6][bb74677f-84f0-4646-a63c-55d831860987], (This step will run and generate new outputs)\n","Created step Model Training [ff56996a][884edd1b-e363-4a0d-b5e9-6d3a90fb0237], (This step will run and generate new outputs)\n","Created step Model Registration [df06b3ce][13374cdd-5994-426e-983f-4c0de03d2c1b], (This step will run and generate new outputs)\n","Created data reference damoseerdata_daf26998 for StepId [999c4b39][a1dc6fcc-1bc5-4644-859b-9cc91cdfb1d7], (Consumers of this data will generate new runs.)\n"]}],"source":"pipeline = Pipeline(workspace=ws, steps=[prepStep, trainStep, registerStep])\n\npublished_pipeline = pipeline.publish(\n    name=\"Seer Pipeline\", \n    description=\"Transfer learned image classifier. Uses folders as labels.\")"},{"cell_type":"code","execution_count":9,"metadata":{"scrolled":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Run created with ID:  7103c1bc-807a-4e25-8874-e578511eb035\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7d9e1953c404a50b50970762b36b757","version_major":2,"version_minor":0},"text/plain":["A Jupyter Widget"]},"metadata":{},"output_type":"display_data"}],"source":"# Submit the pipeline to be run\npipeline_run = Experiment(ws, 'seer',).submit(published_pipeline)\nprint('Run created with ID: ', pipeline_run.id)\n\nRunDetails(pipeline_run).show()"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}